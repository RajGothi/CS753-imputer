# CS753 -  Hacker Role - Imputer

## Motivation

- Fully autoregressive end-to-end ASR methods like RNN-Transducer are slow as they require a total of N steps.
- Non-autoregressive methods like CTC are faster and require only a single step, but their assumption of conditional independence degrades performance.
- Imputer aim to reduce inference time without significantly impacting model performance by utilizing certain characteristics of the ASR task, such as its monotonic alignment. As a result it is able to perform inference in a constant number of steps independent of the sequece length.

## Experiment Overview

## Dataset

- Hugging Face - [Korean Speech Data](https://huggingface.co/datasets/NX2411/AIhub-korean-speech-data)
- Number of samples - 4500 utterances

## Pre-Processing

- Each audio converted to PCM Format and Store the Transcript to text format
- Pre-Processing on the Text and Create the features.
- We have stored the Pre-processed dataset to .mdb (Database) pickel format so we don’t need to pre-process the data again and again on colab.

### Before Training

1. Train a Expert CTC Model.
2. Use this expert model to generate the best alignment for each example in the training set.

### Taining

1. For each training example $(x, y)$, fetch the alignment a generated by the expert model.
2. Use a masking policy to get a masked out alignment ã.
3. Feed the superposition of $ã$ and $x$  to the Imputer, and get the output distribution for posterior alignment.
4. Compute CTC loss, after freezing the unmasked tokens at their positions.

Training Analysis: https://wandb.ai/iitb_asr_cs753/asr?workspace=user-rajgothi6


![MicrosoftTeams-image.png](https://github.com/RajGothi/CS753-imputer/blob/master/Results/MicrosoftTeams-image.png)

# Usage

Imputer loss is basically ctc loss with force emit symbols. (force emit ctc states) So you need to get ctc states. (alignments) You can get it by training ctc models on your data.

Then, you can extract best alignments given input log probabilities and target sequences using `torch_imputer.best_alignment`

```python
def best_alignment(
    log_prob, targets, input_lengths, target_lengths, blank=0, zero_infinity=False
):
    """Get best alignment (maximum probability sequence of ctc states)
       conditioned on log probabilities and target sequences

    Input:
        log_prob (T, N, C): C = number of characters in alphabet including blank
                            T = input length
                            N = batch size
                            log probability of the outputs (e.g. torch.log_softmax of logits)
        targets (N, S): S = maximum number of characters in target sequences
        input_lengths (N): lengths of log_prob
        target_lengths (N): lengths of targets
        blank (int): index of blank tokens (default 0)
        zero_infinity (bool): if true imputer loss will zero out infinities.
                            infinities mostly occur when it is impossible to generate
                            target sequences using input sequences
                            (e.g. input sequences are shorter than target sequences)

    Output:
        best_aligns (List[List[int]]): sequence of ctc states that have maximum probabilties
                                       given log probabilties, and compatible with target sequences"""

```

You can refer to `example/asr/extract_best_align.py`

Then you can train imputer model using `torch_imputer.ImputerLoss` or `torch_imputer.imputer_loss`

```python
def imputer_loss(
    log_prob,
    targets,
    force_emits,
    input_lengths,
    target_lengths,
    blank=0,
    reduction="mean",
    zero_infinity=False,
):
    """The Imputer loss

    Parameters:
        log_prob (T, N, C): C = number of characters in alphabet including blank
                            T = input length
                            N = batch size
                            log probability of the outputs (e.g. torch.log_softmax of logits)
        targets (N, S): S = maximum number of characters in target sequences
        force_emits (N, T): sequence of ctc states that should be occur given times
                            that is, if force_emits is state s at time t, only ctc paths
                            that pass state s at time t will be enabled, and will be zero out the rest
                            this will be same as using cross entropy loss at time t
                            value should be in range [-1, 2 * S + 1), valid ctc states
                            -1 will means that it could be any states at time t (normal ctc paths)
        input_lengths (N): lengths of log_prob
        target_lengths (N): lengths of targets
        blank (int): index of blank tokens (default 0)
        reduction (str): reduction methods applied to the output. 'none' | 'mean' | 'sum'
        zero_infinity (bool): if true imputer loss will zero out infinities.
                              infinities mostly occur when it is impossible to generate
                              target sequences using input sequences
                              (e.g. input sequences are shorter than target sequences)
    """

```

You need to appropriately mask best alignment sequences and pass it `force_emits`. You also need to convert best alignment sequences (that is, sequence of ctc states) into sequence of target tokens to use it as an input to the model. You can do it using function like this:

```python
def get_symbol(state, targets_list):
    """Convert sequence of ctc states into sequence of target tokens

    Input:
        state (List[int]): list of ctc states (e.g. from torch_imputer.best_alignment)
        targets_list (List[int]): token indices of targets
                                  (e.g. targets that you will pass to ctc_loss or imputer_loss)
    """

    if state % 2 == 0:
        symbol = 0

    else:
        symbol = targets_list[state // 2]

    return symbol

```

May you can refer to `collate_data_imputer` in `example/asr/dataset.py` to how you can construct data for imputer loss.

### Possible Implementation
- To Train the Imputer, we first need to train CTC Model and extracting best alignment for train data then we can train the Imputer. It takes so much time to       train the overall model. Come up with different training strategy such that simultaneously we can train both of it, Like GAN does.
- Currently, Imputer Loss Function is written in the C++ library and calling those function in python instead We can implement the Imputer Loss function in the     python from scratch.
- Exploring Different Probability distribution for Masking policy in Imputer Loss and compare the results.



<aside>
<img src="https://www.notion.so/icons/groups_gray.svg" alt="https://www.notion.so/icons/groups_gray.svg" width="40px" />
Team:

22M2160: Raj Kumanbhai Gothi
22M1163: Rahul Kumar
22M1072: Prashant Khatri

</aside>
